{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5307abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchcodec.decoders import AudioDecoder\n",
    "from torchinfo import summary\n",
    "\n",
    "from vox_profile_release.src.model.emotion.whisper_emotion import WhisperWrapper\n",
    "from utils import slice_range, to_nested_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ff4e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ddac021",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 16000\n",
    "MAX_DURATION = 15\n",
    "CHANNELS = 1\n",
    "MAX_LENGTH = SAMPLING_RATE * MAX_DURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c06c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_path = os.path.join('inputs', 'rec_pre.mp3')\n",
    "seg_path = os.path.join('outputs', 'segments.parquet')\n",
    "out_path = os.path.join('outputs', 'segments_emotion')\n",
    "segments = pd.read_parquet(seg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a9a349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of WhisperModel were not initialized from the model checkpoint at openai/whisper-large-v3 and are newly initialized because the shapes did not match:\n",
      "- model.encoder.embed_positions.weight: found shape torch.Size([1500, 1280]) in the checkpoint and torch.Size([750, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = WhisperWrapper.from_pretrained('tiantiaf/whisper-large-v3-msp-podcast-emotion').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d88fc187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperWrapper(\n",
       "  (backbone_model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(750, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (model_seq): Sequential(\n",
       "    (0): Conv1d(1280, 256, kernel_size=(1,), stride=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (emotion_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=9, bias=True)\n",
       "  )\n",
       "  (detailed_out_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=17, bias=True)\n",
       "  )\n",
       "  (arousal_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (valence_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       "  (dominance_layer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a720daa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = AudioDecoder(rec_path, sample_rate=SAMPLING_RATE, num_channels=CHANNELS)\n",
    "audio = [decoder.get_samples_played_in_range(start, end).data.squeeze(0) for start, end, _ in segments.to_numpy()]\n",
    "length = torch.tensor([seg.size(0) for seg in audio], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3badc5-d2ba-4c9a-bfc3-a8cbc019c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = []\n",
    "arousal = []\n",
    "valence = []\n",
    "dominance = []\n",
    "\n",
    "for i, j in slice_range(len(segments.index), 10):\n",
    "    l, _, a, v, d = model.forward(audio[i:j], length=length[i:j], return_feature=False)\n",
    "\n",
    "    logits.append(l.detach().cpu())\n",
    "    arousal.append(a.detach().cpu())\n",
    "    valence.append(v.detach().cpu())\n",
    "    dominance.append(d.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5819ec12-b6ba-4df5-a57e-27ec7e257110",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    segments['emotions.anger'],\n",
    "    segments['emotions.contempt'],\n",
    "    segments['emotions.disgust'],\n",
    "    segments['emotions.fear'],\n",
    "    segments['emotions.happiness'],\n",
    "    segments['emotions.neutral'],\n",
    "    segments['emotions.sadness'],\n",
    "    segments['emotions.surprise'],\n",
    "    segments['emotions.other']\n",
    ") = F.softmax(torch.cat(logits, dim=0), dim=1).numpy().T\n",
    "segments['dimensions.arousal'] = torch.cat(arousal, dim=0).squeeze().numpy()\n",
    "segments['dimensions.valence'] = torch.cat(valence, dim=0).squeeze().numpy()\n",
    "segments['dimensions.dominance'] = torch.cat(dominance, dim=0).squeeze().numpy()\n",
    "segments['text'] = segments.pop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1548c18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.to_parquet(out_path + '.parquet')\n",
    "to_nested_json(segments, out_path + '.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vox-profile",
   "language": "python",
   "name": "vox-profile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
